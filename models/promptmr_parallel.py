import torch
import torch.nn as nn
from typing import List, Tuple, Optional
from .promptmr_v2 import PromptMR, PromptMRBlock, NormPromptUnet, SensitivityModel
from mri_utils import sens_reduce, sens_expand, rss, complex_abs, ifft2c, complex_mul


class ParallelPromptMR(PromptMR):
    """
    å¹¶è¡ŒCascadeæ¶æ„çš„PromptMRå®ç°
    
    æ¶æ„è®¾è®¡:
    Branch A: Cascade1_A â†’ Cascade2_A â†’ Cascade3_A â†’ ... â†’ CascadeN_A
    Branch B: Cascade1_B â†’ Cascade2_B â†’ Cascade3_B â†’ ... â†’ CascadeN_B
    
    Tensorä¼ é€’è·¯å¾„:
    Cascade1_A â†’ Cascade1_B â†’ Cascade2_A â†’ Cascade2_B â†’ Cascade3_A â†’ Cascade3_B â†’ ...
    """
    
    def __init__(
        self,
        num_cascades: int,
        num_adj_slices: int,
        n_feat0: int,
        feature_dim: List[int],
        prompt_dim: List[int],
        sens_n_feat0: int,
        sens_feature_dim: List[int],
        sens_prompt_dim: List[int],
        len_prompt: List[int],
        prompt_size: List[int],
        n_enc_cab: List[int],
        n_dec_cab: List[int],
        n_skip_cab: List[int],
        n_bottleneck_cab: int,
        no_use_ca: bool = False,
        sens_len_prompt: Optional[List[int]] = None,
        sens_prompt_size: Optional[List[int]] = None,
        sens_n_enc_cab: Optional[List[int]] = None,
        sens_n_dec_cab: Optional[List[int]] = None,
        sens_n_skip_cab: Optional[List[int]] = None,
        sens_n_bottleneck_cab: Optional[List[int]] = None,
        sens_no_use_ca: Optional[bool] = None,
        mask_center: bool = True,
        learnable_prompt: bool = False,
        adaptive_input: bool = False,
        n_buffer: int = 4,
        n_history: int = 0,
        use_sens_adj: bool = True,
        parallel_mode: bool = True,  # æ–°å¢å‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
        chain_mode: str = "AB",     # "AB" æˆ– "A_only"
        b_apply_dc: bool = True,
        b_model_scale: float = 1.0,
        b_use_buffer: bool = True,
        b_use_history: bool = True,
        ab_indices: Optional[List[int]] = None,
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼Œä½†ä¸åˆ›å»ºcascades
        super(PromptMR, self).__init__()
        
        self.num_cascades = num_cascades
        self.num_adj_slices = num_adj_slices
        self.center_slice = num_adj_slices // 2
        self.n_buffer = n_buffer
        self.parallel_mode = parallel_mode
        self.chain_mode = chain_mode
        # normalize ab_indices: only allow 0..num_cascades-2 (can't run B after last A)
        raw_indices = ab_indices if ab_indices is not None else []
        norm_indices: List[int] = []
        for idx in raw_indices:
            try:
                iv = int(idx)
                if self.chain_mode == "AB":
                    allow_max = self.num_cascades - 2   # 0..N-2ï¼Œä¿è¯ä»¥Aæ”¶å°¾ï¼Œæ­¥æ•°=2N-1
                else:  # "A_only"
                    allow_max = self.num_cascades - 1   # 0..N-1ï¼Œå…è®¸æœ€åä¸€ä¸ªAåå†è·‘B

                if 0 <= iv <= allow_max:
                    norm_indices.append(iv)
            except Exception:
                continue
        self.ab_indices = sorted(set(norm_indices))
        self._ab_index_set = set(self.ab_indices)
        if not getattr(self, "_ab_debug_printed", False):
            print(f"[ParallelPromptMR] chain_mode={self.chain_mode}, ab_indices={self.ab_indices}")
            self._ab_debug_printed = True
        self._debug_printed = False
        # çµæ•åº¦å›¾ä¼°è®¡ç½‘ç»œ
        self.sens_net = SensitivityModel(
            num_adj_slices=num_adj_slices,
            n_feat0=sens_n_feat0,
            feature_dim=sens_feature_dim,
            prompt_dim=sens_prompt_dim,
            len_prompt=sens_len_prompt if sens_len_prompt is not None else len_prompt,
            prompt_size=sens_prompt_size if sens_prompt_size is not None else prompt_size,
            n_enc_cab=sens_n_enc_cab if sens_n_enc_cab is not None else n_enc_cab,
            n_dec_cab=sens_n_dec_cab if sens_n_dec_cab is not None else n_dec_cab,
            n_skip_cab=sens_n_skip_cab if sens_n_skip_cab is not None else n_skip_cab,
            n_bottleneck_cab=sens_n_bottleneck_cab if sens_n_bottleneck_cab is not None else n_bottleneck_cab,
            no_use_ca=sens_no_use_ca if sens_no_use_ca is not None else no_use_ca,
            mask_center=mask_center,
            learnable_prompt=learnable_prompt,
            use_sens_adj=use_sens_adj
        )
        
        if parallel_mode:
            # åˆ›å»ºä¸¤ä¸ªå¹¶è¡Œçš„cascadeåˆ†æ”¯
            self.cascades_a = nn.ModuleList([
                PromptMRBlock(
                    NormPromptUnet(
                        in_chans=2 * num_adj_slices,
                        out_chans=2 * num_adj_slices,
                        n_feat0=n_feat0,
                        feature_dim=feature_dim,
                        prompt_dim=prompt_dim,
                        len_prompt=len_prompt,
                        prompt_size=prompt_size,
                        n_enc_cab=n_enc_cab,
                        n_dec_cab=n_dec_cab,
                        n_skip_cab=n_skip_cab,
                        n_bottleneck_cab=n_bottleneck_cab,
                        no_use_ca=no_use_ca,
                        learnable_prompt=learnable_prompt,
                        adaptive_input=adaptive_input,
                        n_buffer=n_buffer,
                        n_history=n_history
                    ),
                    num_adj_slices=num_adj_slices
                ) for _ in range(num_cascades)
            ])
            
            self.cascades_b = nn.ModuleList([
                PromptMRBlock(
                    NormPromptUnet(
                        in_chans=2 * num_adj_slices,
                        out_chans=2 * num_adj_slices,
                        n_feat0=n_feat0,
                        feature_dim=feature_dim,
                        prompt_dim=prompt_dim,
                        len_prompt=len_prompt,
                        prompt_size=prompt_size,
                        n_enc_cab=n_enc_cab,
                        n_dec_cab=n_dec_cab,
                        n_skip_cab=n_skip_cab,
                        n_bottleneck_cab=n_bottleneck_cab,
                        no_use_ca=no_use_ca,
                        learnable_prompt=learnable_prompt,
                        adaptive_input=adaptive_input,
                        n_buffer=n_buffer,
                        n_history=n_history
                    ),
                    num_adj_slices=num_adj_slices
                ) for _ in range(num_cascades)
            ])
            
            # é…ç½®Båˆ†æ”¯çš„DCä¸æ¨¡å‹å¼ºåº¦
            for m in self.cascades_b:
                m.apply_dc = b_apply_dc
                m.model_scale = float(b_model_scale)
                m.use_buffer = bool(b_use_buffer)
                m.use_history = bool(b_use_history)
        else:
            # åŸå§‹çš„å•åˆ†æ”¯æ¨¡å¼
            self.cascades = nn.ModuleList([
                PromptMRBlock(
                    NormPromptUnet(
                        in_chans=2 * num_adj_slices,
                        out_chans=2 * num_adj_slices,
                        n_feat0=n_feat0,
                        feature_dim=feature_dim,
                        prompt_dim=prompt_dim,
                        len_prompt=len_prompt,
                        prompt_size=prompt_size,
                        n_enc_cab=n_enc_cab,
                        n_dec_cab=n_dec_cab,
                        n_skip_cab=n_skip_cab,
                        n_bottleneck_cab=n_bottleneck_cab,
                        no_use_ca=no_use_ca,
                        learnable_prompt=learnable_prompt,
                        adaptive_input=adaptive_input,
                        n_buffer=n_buffer,
                        n_history=n_history
                    ),
                    num_adj_slices=num_adj_slices
                ) for _ in range(num_cascades)
            ])
    
    def copy_weights_from_original(self, original_model):
        """
        ä»åŸå§‹PromptMRæ¨¡å‹å¤åˆ¶æƒé‡åˆ°å¹¶è¡Œæ¶æ„
        
        Args:
            original_model: åŸå§‹çš„PromptMRæ¨¡å‹å®ä¾‹
        """
        if not self.parallel_mode:
            raise ValueError("åªæœ‰åœ¨parallel_mode=Trueæ—¶æ‰èƒ½å¤åˆ¶æƒé‡")
        
        print("æ­£åœ¨å¤åˆ¶æƒé‡åˆ°å¹¶è¡Œæ¶æ„...")
        
        # å¤åˆ¶çµæ•åº¦å›¾ç½‘ç»œæƒé‡
        self.sens_net.load_state_dict(original_model.sens_net.state_dict())
        
        # å¤åˆ¶cascadeæƒé‡åˆ°ä¸¤ä¸ªåˆ†æ”¯
        for i in range(self.num_cascades):
            # å¤åˆ¶åˆ°åˆ†æ”¯A
            self.cascades_a[i].load_state_dict(original_model.cascades[i].state_dict())
            # å¤åˆ¶åˆ°åˆ†æ”¯B  
            self.cascades_b[i].load_state_dict(original_model.cascades[i].state_dict())
        
        print(f"æˆåŠŸå¤åˆ¶äº†{self.num_cascades}ä¸ªcascadeçš„æƒé‡åˆ°ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯")
    
    def forward_parallel(
        self,
        masked_kspace: torch.Tensor,
        mask: torch.Tensor,
        num_low_frequencies: torch.Tensor,
        mask_type: Tuple[str] = ("cartesian",),
        use_checkpoint: bool = False,
        compute_sens_per_coil: bool = False,
    ) -> torch.Tensor:
        """
        å¹¶è¡Œcascadeçš„å‰å‘ä¼ æ’­ (å…¼å®¹promptmr_v2æ¥å£)
        
        Tensorä¼ é€’è·¯å¾„:
        Cascade1_A â†’ Cascade1_B â†’ Cascade2_A â†’ Cascade2_B â†’ Cascade3_A â†’ Cascade3_B â†’ ...
        """
        # è®¡ç®—çµæ•åº¦å›¾
        if use_checkpoint:
            sens_maps = torch.utils.checkpoint.checkpoint(
                self.sens_net, masked_kspace, mask, num_low_frequencies, mask_type, compute_sens_per_coil,
                use_reentrant=False)
        else:
            sens_maps = self.sens_net(masked_kspace, mask, num_low_frequencies, mask_type, compute_sens_per_coil)

        img_zf = sens_reduce(masked_kspace, sens_maps, self.num_adj_slices)
        img_pred = img_zf.clone()
        latent = img_zf.clone()
        history_feat = None

        # å¹¶è¡Œcascadeå¤„ç†
        total_steps = 0
        for ith in range(self.num_cascades):
            is_last = ith == self.num_cascades - 1
            
            # åˆ†æ”¯Aå¤„ç†
            if use_checkpoint and self.training:
                img_pred, latent, history_feat = torch.utils.checkpoint.checkpoint(
                    self.cascades_a[ith], img_pred, img_zf, latent, mask, sens_maps, history_feat, 
                    use_reentrant=False)
            else:
                img_pred, latent, history_feat = self.cascades_a[ith](
                    img_pred, img_zf, latent, mask, sens_maps, history_feat)
            total_steps += 1
            
            # åˆ†æ”¯Bå¤„ç† (å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªcascade)
            run_b_this_step = (
                (self.chain_mode == "AB" and not is_last) or
                (self.chain_mode == "A_only" and ith in self._ab_index_set)
            )
            if run_b_this_step:
                if use_checkpoint and self.training:
                    img_pred, latent, history_feat = torch.utils.checkpoint.checkpoint(
                        self.cascades_b[ith], img_pred, img_zf, latent, mask, sens_maps, history_feat, 
                        use_reentrant=False)
                else:
                    img_pred, latent, history_feat = self.cascades_b[ith](
                        img_pred, img_zf, latent, mask, sens_maps, history_feat)
                total_steps += 1
        
        # è°ƒè¯•ä¿¡æ¯ï¼šç¡®è®¤æ‰§è¡Œæ­¥éª¤æ•°
        if hasattr(self, '_debug_printed') and not self._debug_printed:
            print(f"ğŸ” å¹¶è¡Œæ¶æ„æ‰§è¡Œäº† {total_steps} ä¸ªcascadeæ­¥éª¤ (é¢„æœŸ: {self.num_cascades * 2 - 1})")
            self._debug_printed = True

        # è·å–æœ€ç»ˆè¾“å‡º
        current_kspace = sens_expand(img_pred, sens_maps, self.num_adj_slices)
        img_pred = torch.chunk(img_pred, self.num_adj_slices, dim=1)[self.center_slice]
        pred_kspace = torch.chunk(current_kspace, self.num_adj_slices, dim=1)[self.center_slice]
        
        # å¤„ç†img_predä¸ºå•çº¿åœˆå›¾åƒ (ä¸promptmr_v2.pyä¿æŒä¸€è‡´)
        sens_maps = torch.chunk(sens_maps, self.num_adj_slices, dim=1)[self.center_slice]
        img_pred = rss(complex_abs(complex_mul(img_pred, sens_maps)), dim=1)
        
        # å‡†å¤‡é¢å¤–è¾“å‡º (ä¸promptmr_v2.pyä¿æŒä¸€è‡´)
        kspace_zf = masked_kspace
        img_zf = torch.chunk(masked_kspace, self.num_adj_slices, dim=1)[self.center_slice]
        kspace_zf = torch.chunk(masked_kspace, self.num_adj_slices, dim=1)[self.center_slice]
        img_zf = rss(complex_abs(ifft2c(img_zf)), dim=1)
        
        sens_maps = torch.view_as_complex(sens_maps)
        
        return {
            'img_pred': img_pred,
            'img_zf': img_zf,
            'sens_maps': sens_maps,
            'pred_kspace': pred_kspace,
            'original_kspace': kspace_zf  # ä¿®å¤ï¼šä¸promptmr_v2.pyä¿æŒä¸€è‡´
        }
    
    def forward(
        self,
        masked_kspace: torch.Tensor,
        mask: torch.Tensor,
        num_low_frequencies: torch.Tensor,
        mask_type: Tuple[str] = ("cartesian",),
        use_checkpoint: bool = False,
        compute_sens_per_coil: bool = False,
    ) -> torch.Tensor:
        """
        ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æ¥å£
        """
        if self.parallel_mode:
            return self.forward_parallel(masked_kspace, mask, num_low_frequencies, mask_type, use_checkpoint, compute_sens_per_coil)
        else:
            # è°ƒç”¨çˆ¶ç±»çš„åŸå§‹forwardæ–¹æ³•
            return super().forward(masked_kspace, mask, num_low_frequencies, mask_type, use_checkpoint, compute_sens_per_coil)


def create_parallel_promptmr_from_checkpoint(checkpoint_path: str, parallel_mode: bool = True):
    """
    ä»checkpointåˆ›å»ºå¹¶è¡ŒPromptMRæ¨¡å‹
    
    Args:
        checkpoint_path: åŸå§‹æ¨¡å‹checkpointè·¯å¾„
        parallel_mode: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
    
    Returns:
        ParallelPromptMR: å¹¶è¡Œæ¶æ„çš„æ¨¡å‹
    """
    # åŠ è½½åŸå§‹checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # æå–æ¨¡å‹é…ç½®
    if 'hyper_parameters' in checkpoint:
        config = checkpoint['hyper_parameters']
    else:
        # å¦‚æœcheckpointä¸­æ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
        config = {
            'num_cascades': 12,
            'num_adj_slices': 5,
            'n_feat0': 48,
            'feature_dim': [72, 96, 120],
            'prompt_dim': [24, 48, 72],
            'sens_n_feat0': 24,
            'sens_feature_dim': [36, 48, 60],
            'sens_prompt_dim': [12, 24, 36],
            'len_prompt': [5, 5, 5],
            'prompt_size': [64, 32, 16],
            'n_enc_cab': [2, 3, 3],
            'n_dec_cab': [2, 2, 3],
            'n_skip_cab': [1, 1, 1],
            'n_bottleneck_cab': 3,
            'no_use_ca': False,
            'learnable_prompt': False,
            'adaptive_input': True,
            'n_buffer': 4,
            'n_history': 0,
            'use_sens_adj': True,
        }
    
    # åˆ›å»ºå¹¶è¡Œæ¨¡å‹
    parallel_model = ParallelPromptMR(
        parallel_mode=parallel_mode,
        **config
    )
    
    # å¦‚æœä½¿ç”¨å¹¶è¡Œæ¨¡å¼ï¼Œå¤åˆ¶æƒé‡
    if parallel_mode and 'state_dict' in checkpoint:
        # å…ˆåˆ›å»ºåŸå§‹æ¨¡å‹æ¥æå–æƒé‡
        original_model = PromptMR(**config)
        original_model.load_state_dict(checkpoint['state_dict'], strict=False)
        
        # å¤åˆ¶æƒé‡åˆ°å¹¶è¡Œæ¶æ„
        parallel_model.copy_weights_from_original(original_model)
    
    return parallel_model

