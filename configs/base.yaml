# common settings for all experiments
seed_everything: 42
trainer:
  accelerator: gpu
  strategy: ddp
  gradient_clip_val: 0.1
  log_every_n_steps: 50 # log every n steps during training
  deterministic: false
  use_distributed_sampler: false # use customized distributed sampler defined in data module while validation
  logger: 
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: deep_recon
      mode: online
  callbacks:
    -
      class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: val_loss
        mode: min 
        save_top_k: -1 # -1 to save all checkpoints, 5 to save the top 5 checkpoints
        save_last: true # always save the last checkpoint
        verbose: true # print checkpoint information
        save_on_train_epoch_end: null
    # - 
    #   class_path: lightning.pytorch.callbacks.LearningRateMonitor
    #   init_args:
    #     logging_interval: 'epoch'
    #     log_momentum: true
    #     log_weight_decay: true

model:
  class_path: pl_modules.PromptDDPMModule
  init_args: {}